---
layout: post
tags: [运维, linux, CentOS, 虚拟机, k8s, kubernetes, containerd]
---

服务器密码 mfhk12xx

服务启动失败，通过指令查看最近报错

```
journalctl -xe
```

 也可通过指令查看最近服务的日志

```
tail -f /var/log/messages
```

查看ETCD健康状态

```
ETCD_PREFIX='ETCDCTL_API=3 etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints="https://192.168.100.100:2379,https://192.168.100.101:2379,https://192.168.100.102:2379" --write-out=table'
eval "${ETCD_PREFIX} endpoint health"
```

查看master组件健康状态

```
systemctl status kube-proxy
systemctl status kubelet
systemctl status kube-scheduler
systemctl status kube-controller-manager
systemctl status kube-apiserver
kubectl get cs
```



# 前置操作

## 搞奇数台linux服务器

怎么搞的略，我搞了三个

|    host    |       ip        |
| :--------: | :-------------: |
| k8s-master | 192.168.100.100 |
| k8s-node1  | 192.168.100.101 |
| k8s-node2  | 192.168.100.102 |



## 分配好主从节点

每一台服务器都执行

```
cat >> /etc/hosts <<EOF
192.168.100.100 k8s-master
192.168.100.101 k8s-node1
192.168.100.102 k8s-node2
EOF

```

## 关闭防火墙

这样的比较方便

```
systemctl stop firewalld
systemctl disable firewalld

```

## 关闭swap

```
sed -ri 's/.*swap.*/#&/' /etc/fstab #永久
#swapoff -a   #临时

```

## 关闭selinux

```
sed -i 's/enforcing/disabled/' /etc/selinux/config  #永久
#setenforce 0  # 临时 

```



## 桥接流量转发

每一台服务器都执行

```
cat > /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv6.conf.all.disable_ipv6=1
net.ipv4.ip_forward=1
EOF
sysctl --system

```

## 时间同步

每一台服务器都执行

```
yum install ntpdate -y
ntpdate time.windows.com

```

## 主机名规划

按照规划在对应ip的服务器执行对应的set-hostname

```
hostnamectl set-hostname k8s-master 
hostnamectl set-hostname k8s-node1
hostnamectl set-hostname k8s-node2

```

## 服务部署

|       ip        |    host    |                             服务                             |
| :-------------: | :--------: | :----------------------------------------------------------: |
| 192.168.100.100 | k8s-master | kube-proxy<br/>kubelet<br/>kube-scheduler<br/>kube-controller-manager<br/>kube-apiserver |
| 192.168.100.101 | k8s-node1  |                    kube-proxy<br/>kubelet                    |
| 192.168.100.102 | k8s-node2  |                    kube-proxy<br/>kubelet                    |



# 部署ETCD

## 下载cfssl

在某一台服务器上下载就行了

```
wget https://github.com/cloudflare/cfssl/releases/download/v1.6.3/cfssl_1.6.3_linux_amd64 -O /usr/local/bin/cfssl
wget https://github.com/cloudflare/cfssl/releases/download/v1.6.3/cfssljson_1.6.3_linux_amd64 -O /usr/local/bin/cfssljson
wget https://github.com/cloudflare/cfssl/releases/download/v1.6.3/cfssl-certinfo_1.6.3_linux_amd64 -O /usr/local/bin/cfssl-certinfo

chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson /usr/local/bin/cfssl-certinfo 

cp cfssl_1.6.3_linux_amd64 cfssl
cp cfssl-certinfo_1.6.3_linux_amd64 cfssl-certinfo
cp cfssljson_1.6.3_linux_amd64 cfssljson
```

wget下载不下来可以手动下载

## ETCD证书创建

在下载了cfssl的服务器上执行

```
mkdir /opt/etcd/{bin,cfg,ssl} -p
cd /opt/etcd/ssl/
```

```
vi ca-config.json
```

```
{
    "signing": {
        "default": {
            "expiry": "87600h"
        },
        "profiles": {
            "etcd": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}
```

```
vi ca-csr.json
```

```
{
    "CN": "etcd CA",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "Beijing",
            "L": "Beijing"
        }
    ]
}
```

```
vi server-csr.json
```

下面的hosts需要改成自己的

```
{
    "CN": "etcd",
    "hosts": [
         "192.168.100.100",
         "192.168.100.101",
         "192.168.100.102"
        ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "Beijing",
            "L": "Beijing"
        }
    ]
}
```

生成

```
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
```

```
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd server-csr.json | cfssljson -bare server
```

证书拷贝到所有节点的相同位置，主要是ca.pem、server.pem、server-key.pem三个文件，我放在/opt/etcd/ssl/内部

## ETCD集群搭建

在一台服务器下载etcd

```
ETCD_VER=v3.4.24

//这两个都可以
GOOGLE_URL=https://storage.googleapis.com/etcd
GITHUB_URL=https://github.com/etcd-io/etcd/releases/download
DOWNLOAD_URL=${GOOGLE_URL}

rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz
rm -rf /tmp/etcd-download-test && mkdir -p /tmp/etcd-download-test

curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz
tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1
rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz
```

```
cd /tmp/etcd-download-test/
cp etcd etcdctl /usr/local/bin/
```

这里的etcd、etcdctl两个文件拷贝到全部节点的/usr/local/bin/目录内

对所有节点进行下面的操作，配置文件注意改一部分

---

```
vi /opt/etcd/cfg/etcd.conf
```

配置文件

```
ETCD_INITIAL_CLUSTER_STATEETCD_INITIAL_CLUSTER_STATE#[Member]
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_NAME="etcd01"#修改为ETCD_INITIAL_CLUSTER中=右边为本机ip的左边的name
ETCD_LISTEN_PEER_URLS="https://192.168.100.100:2380" #修改为本机ip
ETCD_LISTEN_CLIENT_URLS="https://192.168.100.100:2379,http://127.0.0.1:2379"#修改为本机ip
#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.100.100:2380"#修改为本机ip
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.100.100:2379"#修改为本机ip

#下面的名字跟ip需要事先规划下
ETCD_INITIAL_CLUSTER="etcd01=https://192.168.100.100:2380,etcd02=https://192.168.100.101:2380,etcd03=https://192.168.100.102:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
#[Security]

#这里是前面cfssl创建的证书，用于开启TLS
ETCD_CERT_FILE="/opt/etcd/ssl/server.pem"
ETCD_KEY_FILE="/opt/etcd/ssl/server-key.pem"
ETCD_TRUSTED_CA_FILE="/opt/etcd/ssl/ca.pem"
ETCD_CLIENT_CERT_AUTH="true"
ETCD_PEER_CERT_FILE="/opt/etcd/ssl/server.pem"
ETCD_PEER_KEY_FILE="/opt/etcd/ssl/server-key.pem"
ETCD_PEER_TRUSTED_CA_FILE="/opt/etcd/ssl/ca.pem"
ETCD_PEER_CLIENT_CERT_AUTH="true"
```

字段介绍，下面的代码块是介绍，看看就行

```
--initial-cluster：集群当中的其他节点
--cert-file：etcd证书路径
--key-file：etcd私钥路径
--peer-cert-file：对等证书(双向证书)路径
--peer-key-file：对等证书(双向证书)私钥路径
--trusted-ca-file：作为客户端时的CA证书路径
--peer-trusted-ca-file：对等证书的CA证书路径
--initial-advertise-peer-urls：列出集群成员通信的URL，用于通告集群其他成员
--listen-peer-urls：用于监听集群其他成员的URL列表
--listen-client-urls：用于监听客户端通讯的URL列表
--advertise-client-urls：通告客户端的URL，用于列出所有客户端
--initial-cluster-token：etcd集群的初始集群令牌，服务器必须通过令牌才能加入etcd集群
```

```
vi /usr/lib/systemd/system/etcd.service
```

```
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
EnvironmentFile=-/opt/etcd/cfg/etcd.conf
ExecStart=/usr/local/bin/etcd
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
```

然后启动服务，组成集群的几个节点需要同时开启etcd服务，从上到下的指令分别是：

读取服务文件、开机启动etcd、开启etcd服务、查看etcd运行状态

```
systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
systemctl status etcd

```

运行状态是running就行

这里的服务启动要同时，ETCD_INITIAL_CLUSTER配置有几台就要在几台启动。

服务启动要是报错了有提示、可以百度看看哪里有问题

---

到这里就部署完毕了，后面是对部署结果的查看

使用脚手架的固定前缀，证书位置自己定，服务器ip自己改

```
ETCD_PREFIX='ETCDCTL_API=3 etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints="https://192.168.100.100:2379,https://192.168.100.101:2379,https://192.168.100.102:2379" --write-out=table'
```

查看成员信息

```
eval "${ETCD_PREFIX} member list"
```

查看集群状态

```
eval "${ETCD_PREFIX} endpoint health"
```

全部的health列都是true表示部署完成，这个时候如果有某一个health是false，重启那一个etcd。

重启前修改配置的ETCD_INITIAL_CLUSTER_STATE为"existing"，然后执行下面的指令即可

```
systemctl restart etcd
```

# 部署Containerd

全部节点按照下面的步骤进行container部署

```
mkdir /opt/{containerd,cni/bin} -p
```

下container的载gz安装包跟cni的gz安装包还有runc的二进制包，这里container使用1.7.0版本，cni使用1.2.0，runc使用1.1.4

containerd下载

```
https://github.com/containerd/containerd/releases/download/v1.7.0/containerd-1.7.0-linux-amd64.tar.gz
```

cni插件

```
https://github.com/containernetworking/plugins/releases/download/v1.2.0/cni-plugins-linux-amd64-v1.2.0.tgz
```

runc插件

```
https://github.com/opencontainers/runc/releases/download/v1.1.4/runc.amd64
```

containerd文件放到/opt/containerd解压，ctr放到系统环境

```
cp /opt/containerd/bin/ctr  /usr/local/bin
```

cni放到/opt/cni/bin解压

解压命令

```
tar -zxf *gz
```

在放置runc.amd64的地方运行

```
install -m 755 runc.amd64 /usr/local/sbin/runc
```

先启动一次

```
cat > /etc/systemd/system/containerd.service <<EOF
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target local-fs.target
 
[Service]
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/opt/containerd/bin/containerd
Type=notify
Delegate=yes
KillMode=process
Restart=always
RestartSec=5
LimitNPROC=infinity
LimitCORE=infinity
LimitNOFILE=infinity
TasksMax=infinity
OOMScoreAdjust=-999
 
[Install]
WantedBy=multi-user.target
EOF
systemctl daemon-reload
systemctl enable containerd
systemctl start containerd
systemctl status containerd
```

配置Containerd所需的模块

```
cat > /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
systemctl restart systemd-modules-load.service
```

配置Containerd所需的内核

```
cat >> /etc/sysctl.d/99-kubernetes-cri.conf << EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
sysctl --system
```

创建cni配置文件

```
mkdir /etc/cni/net.d/ -p
cat > /etc/cni/net.d/10-mynet.conf <<EOF
{
    "cniVersion": "1.0.0",
    "name": "mynet",
    "type": "bridge",
    "bridge": "cni0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "subnet": "10.88.0.0/16",
        "routes": [
            {
                "dst": "0.0.0.0/0"
            }
        ]
    }
}
EOF
cat > /etc/cni/net.d/99-loopback.conf <<EOF
{
    "cniVerion": "1.0.0",
    "name": "lo",
    "type": "loopback"
}
EOF
```

创建Containerd的配置文件

```
mkdir /etc/containerd
/opt/containerd/bin/containerd config default | tee /etc/containerd/config.toml
```

 **修改Containerd的配置文件/etc/containerd/config.toml，几个配置是分开的**

```
#这个配置里的[plugins."io.containerd.grpc.v1.cri"]
sandbox_image = "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6"
#这个配置里的[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options] SystemdCgroup注释掉
# SystemdCgroup = true
```

查看配置里的这一项config_path，没有自己加

```
[plugins."io.containerd.grpc.v1.cri".registry]
  config_path = "/etc/containerd/certs.d"
```

查看命名空间

```
ctr namespaces ls
```

创建命名空间的配置

```
mkdir /etc/containerd/certs.d/k8s.io -p
```

配置镜像加速host.后面的改成自己阿里云的免费镜像加速地址

```
cat >  /etc/containerd/certs.d/k8s.io/hosts.toml  <<EOF
server = "https://k8s.io"

[host."https://d68wvxw0.mirror.aliyuncs.com"]
  capabilities = ["pull", "resolve", "push"]
EOF
```

启动测试

```
systemctl restart containerd
ctr version

```

## 

# 主节点部署Master组件

## 下载源码

这里有全部版本的日志，进去这个页面下滑有很多版本

```
https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG
```

点击版本进去后就能看到版本的下载链接

点击Download for xxx，进去找到Server Binaries，是Server，下载第一个kubernetes-server-linux-amd64.tar.gz

我下载的1.26.3版本，之后到规划的Master服务器跑代码

规划一个放k8s的文件夹

```
mkdir -p /opt/kubernetes/{bin,cfg,ssl,logs} 
```

将下载的gz文件放在/opt/kubernetes/目录下

```
cd /opt/kubernetes
tar zxvf kubernetes-server-linux-amd64.tar.gz
cd kubernetes/server/bin
cp kube-apiserver kube-scheduler kube-controller-manager kubelet kube-proxy kubectl kubeadm /opt/kubernetes/bin
cp kubectl kubeadm /usr/bin/
```

## 部署kube-apiserver

```
mkdir /opt/kubernetes/ssl/kube-apiserver

cd /opt/kubernetes/ssl/kube-apiserver


cat > ca-config.json << EOF
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "kubernetes": {
         "expiry": "87600h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}
EOF
cat > ca-csr.json << EOF
{
    "CN": "kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
EOF

cat > server-csr.json << EOF
{
    "CN": "kubernetes",
    "hosts": [
      "127.0.0.1",
      "192.168.100.100",
      "192.168.100.101",
      "192.168.100.102",
      "k8s-master",
      "k8s-node1",
      "k8s-node2"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
EOF

```

生成证书

```
cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server

```

创建配置

```
cat > /opt/kubernetes/cfg/kube-apiserver.conf << EOF
KUBE_APISERVER_OPTS="--v=2 \\
--etcd-servers=https://192.168.100.100:2379,https://192.168.100.101:2379,https://192.168.100.102:2379 \\
--etcd-cafile=/opt/etcd/ssl/ca.pem \\
--etcd-certfile=/opt/etcd/ssl/server.pem \\
--etcd-keyfile=/opt/etcd/ssl/server-key.pem \\
--bind-address=192.168.100.100 \\
--secure-port=6443 \\
--advertise-address=192.168.100.100 \\
--allow-privileged=true \\
--authorization-mode=RBAC,Node \\
--token-auth-file=/opt/kubernetes/cfg/token.csv \\
--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \\
--enable-bootstrap-token-auth=true \\
--service-cluster-ip-range=10.0.0.0/24 \\
--service-node-port-range=30000-32767 \\
--service-account-issuer=api \\
--service-account-key-file=/opt/kubernetes/ssl/kube-apiserver/ca-key.pem \\
--service-account-signing-key-file=/opt/kubernetes/ssl/kube-apiserver/server-key.pem \\
--proxy-client-cert-file=/opt/kubernetes/ssl/kube-apiserver/server.pem \\
--proxy-client-key-file=/opt/kubernetes/ssl/kube-apiserver/server-key.pem \\
--kubelet-client-certificate=/opt/kubernetes/ssl/kube-apiserver/server.pem \\
--kubelet-client-key=/opt/kubernetes/ssl/kube-apiserver/server-key.pem \\
--tls-cert-file=/opt/kubernetes/ssl/kube-apiserver/server.pem  \\
--tls-private-key-file=/opt/kubernetes/ssl/kube-apiserver/server-key.pem \\
--client-ca-file=/opt/kubernetes/ssl/kube-apiserver/ca.pem \\
--requestheader-client-ca-file=/opt/kubernetes/ssl/kube-apiserver/ca.pem \\
--requestheader-allowed-names=kubernetes \\
--requestheader-extra-headers-prefix=X-Remote-Extra- \\
--requestheader-group-headers=X-Remote-Group \\
--requestheader-username-headers=X-Remote-User \\
--enable-aggregator-routing=true \\
--audit-log-maxage=30 \\
--audit-log-maxbackup=3 \\
--audit-log-maxsize=100 \\
--audit-log-path=/opt/kubernetes/logs/k8s-audit.log"
EOF

```

生成一个token

```
head -c 16 /dev/urandom | od -An -t x | tr -d ' '

```

写入bootstraping验证token的文件

```
cat > /opt/kubernetes/cfg/token.csv << EOF
dcbfd5e168fde8abda5ed8b0fda3291f,kubelet-bootstrap,10001,"system:node-bootstrapper"
EOF

```

创建apiserver服务

```
cat > /usr/lib/systemd/system/kube-apiserver.service << EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-apiserver.conf
ExecStart=/opt/kubernetes/bin/kube-apiserver \$KUBE_APISERVER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

```

```
systemctl daemon-reload
systemctl enable kube-apiserver
systemctl start kube-apiserver 
systemctl status kube-apiserver 

```

状态是running就ok

## 部署kube-controller-manager

创建配置

```
cat > /opt/kubernetes/cfg/kube-controller-manager.conf << EOF
KUBE_CONTROLLER_MANAGER_OPTS="--v=2 \\
--leader-elect=true \\
--kubeconfig=/opt/kubernetes/cfg/kube-controller-manager.kubeconfig \\
--bind-address=127.0.0.1 \\
--allocate-node-cidrs=true \\
--cluster-cidr=10.244.0.0/16 \\
--service-cluster-ip-range=10.0.0.0/24 \\
--cluster-signing-cert-file=/opt/kubernetes/ssl/kube-apiserver/ca.pem \\
--cluster-signing-key-file=/opt/kubernetes/ssl/kube-apiserver/ca-key.pem  \\
--root-ca-file=/opt/kubernetes/ssl/kube-apiserver/ca.pem \\
--service-account-private-key-file=/opt/kubernetes/ssl/kube-apiserver/ca-key.pem \\
--cluster-signing-duration=87600h0m0s"
EOF

```

生成证书

```
mkdir /opt/kubernetes/ssl/kube-controller-manager
cd /opt/kubernetes/ssl/kube-controller-manager
cp /opt/kubernetes/ssl/kube-apiserver/ca* /opt/kubernetes/ssl/kube-controller-manager
cat > kube-controller-manager-csr.json << EOF
{
  "CN": "system:kube-controller-manager",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing", 
      "ST": "BeiJing",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager

```

shell执行以下命令生成kubeconfig文件

```
KUBE_CONFIG="/opt/kubernetes/cfg/kube-controller-manager.kubeconfig"
KUBE_APISERVER="https://192.168.100.100:6443"

kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/kubernetes/ssl/kube-apiserver/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-credentials kube-controller-manager \
  --client-certificate=/opt/kubernetes/ssl/kube-controller-manager/kube-controller-manager.pem \
  --client-key=/opt/kubernetes/ssl/kube-controller-manager/kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-controller-manager \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

```

创建服务

```
cat > /usr/lib/systemd/system/kube-controller-manager.service << EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-controller-manager.conf
ExecStart=/opt/kubernetes/bin/kube-controller-manager \$KUBE_CONTROLLER_MANAGER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

```

启动

```
systemctl daemon-reload
systemctl enable kube-controller-manager
systemctl start kube-controller-manager
systemctl status kube-controller-manager

```

状态是running就ok

## 部署 kube-scheduler

创建配置

```
cat > /opt/kubernetes/cfg/kube-scheduler.conf << EOF
KUBE_SCHEDULER_OPTS="--v=2 \\
--leader-elect \\
--kubeconfig=/opt/kubernetes/cfg/kube-scheduler.kubeconfig \\
--bind-address=127.0.0.1"
EOF

```

生成证书

```
mkdir /opt/kubernetes/ssl/kube-scheduler
cd /opt/kubernetes/ssl/kube-scheduler
cp /opt/kubernetes/ssl/kube-apiserver/ca* /opt/kubernetes/ssl/kube-scheduler
# 创建证书请求文件
cat > kube-scheduler-csr.json << EOF
{
  "CN": "system:kube-scheduler",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "ST": "BeiJing",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF
# 生成证书
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler

```

生成

```
KUBE_CONFIG="/opt/kubernetes/cfg/kube-scheduler.kubeconfig"
KUBE_APISERVER="https://192.168.100.100:6443"

kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/kubernetes/ssl/kube-apiserver/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-credentials kube-scheduler \
  --client-certificate=/opt/kubernetes/ssl/kube-scheduler/kube-scheduler.pem \
  --client-key=/opt/kubernetes/ssl/kube-scheduler/kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-scheduler \
  --kubeconfig=${KUBE_CONFIG}
    
kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

```

创建服务管理

```
cat > /usr/lib/systemd/system/kube-scheduler.service << EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-scheduler.conf
ExecStart=/opt/kubernetes/bin/kube-scheduler \$KUBE_SCHEDULER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

```

启动

```
systemctl daemon-reload
systemctl enable kube-scheduler
systemctl start kube-scheduler
systemctl status kube-scheduler

```

## 集群管理前配置

生成集群链接证书

```
mkdir /opt/kubernetes/ssl/kube-link
cd /opt/kubernetes/ssl/kube-link
cp /opt/kubernetes/ssl/kube-apiserver/ca* /opt/kubernetes/ssl/kube-link

cat > admin-csr.json <<EOF
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "ST": "BeiJing",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin

```

创建kubeconfig文件

```
mkdir /root/.kube

KUBE_CONFIG="/root/.kube/config"
KUBE_APISERVER="https://192.168.100.100:6443"

kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/kubernetes/ssl/kube-link/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-credentials cluster-admin \
  --client-certificate=/opt/kubernetes/ssl/kube-link/admin.pem \
  --client-key=/opt/kubernetes/ssl/kube-link/admin-key.pem \
  --embed-certs=true \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user=cluster-admin \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

```

查看节点组件状态

```
 kubectl get cs
```

health全是true就是组件正常

创建用户

```
kubectl create clusterrolebinding kubelet-bootstrap \
--clusterrole=system:node-bootstrapper \
--user=kubelet-bootstrap

```

# 集群全部节点部署

## 安装ctictl

下载

```
https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.26.1/crictl-v1.26.1-linux-amd64.tar.gz
```

解压后移动到环境变量里

```
tar -zxvf crictl-v1.26.1-linux-amd64.tar.gz
mv crictl /usr/local/bin/
```

添加crictl配置

```
cat > /etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 2
debug: false
pull-image-on-create: false
disable-pull-on-run: false
EOF

```

## （master）部署kubelet

先在master节点按照下面的步骤kubelet部署

```
cat > /opt/kubernetes/cfg/kubelet-config.yml << EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS:
- 10.0.0.2
clusterDomain: k8s-master 
failSwapOn: false
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /opt/kubernetes/ssl/kube-apiserver/ca.pem 
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
maxOpenFiles: 1000000
maxPods: 110
EOF

```

创建bootstrap.kubeconfig

```
KUBE_CONFIG="/opt/kubernetes/cfg/bootstrap.kubeconfig"
KUBE_APISERVER="https://192.168.100.100:6443" 
TOKEN="dcbfd5e168fde8abda5ed8b0fda3291f"

kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/kubernetes/ssl/kube-apiserver/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-credentials "kubelet-bootstrap" \
  --token=${TOKEN} \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user="kubelet-bootstrap" \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

```

```
cat > /opt/kubernetes/cfg/kubelet.conf << EOF
KUBELET_OPTS="--hostname-override=k8s-master \\
--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\
--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\
--config=/opt/kubernetes/cfg/kubelet-config.yml \\
--cert-dir=/opt/kubernetes/ssl/kube-link \\
--container-runtime-endpoint=unix:///run/containerd/containerd.sock"
EOF

```

创建服务

```
cat > /usr/lib/systemd/system/kubelet.service << EOF
[Unit]
Description=Kubernetes Kubelet
After=docker.service

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kubelet.conf
ExecStart=/opt/kubernetes/bin/kubelet \$KUBELET_OPTS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

```

```
systemctl daemon-reload
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet

```

允许kubelet证书申请并加入集群

```
kubectl get csr
#上面的命令执行后的表格，取name那一块作为下面命令的参数
kubectl certificate approve node-csr-t8Gq2LXx4SJ-WDa-HFeuc0ESNvEZ8oMTsZcaybs1NII
#执行下面的命令能看到k8s-master展示列表状态为ready
kubectl get nodes

```

## （master）部署kube-proxy

创建配置

```
cat > /opt/kubernetes/cfg/kube-proxy.conf << EOF
KUBE_PROXY_OPTS="--v=2 \\
--config=/opt/kubernetes/cfg/kube-proxy-config.yml"
EOF
```

```
cat > /opt/kubernetes/cfg/kube-proxy-config.yml << EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
metricsBindAddress: 0.0.0.0:10249
clientConnection:
  kubeconfig: /opt/kubernetes/cfg/kube-proxy.kubeconfig
hostnameOverride: k8s-master
clusterCIDR: 10.244.0.0/16
EOF
```

创建证书

```
mkdir /opt/kubernetes/ssl/kube-proxy
cd /opt/kubernetes/ssl/kube-proxy
cp /opt/kubernetes/ssl/kube-apiserver/ca* /opt/kubernetes/ssl/kube-proxy
cat > kube-proxy-csr.json << EOF
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "ST": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy

```

生成kube-proxy.kubeconfig文件

```
KUBE_CONFIG="/opt/kubernetes/cfg/kube-proxy.kubeconfig"
KUBE_APISERVER="https://192.168.100.100:6443"
CERT_PATH="/opt/kubernetes/ssl/kube-proxy"

kubectl config set-cluster kubernetes \
  --certificate-authority=${CERT_PATH}/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-credentials kube-proxy \
  --client-certificate=${CERT_PATH}/kube-proxy.pem \
  --client-key=${CERT_PATH}/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=${KUBE_CONFIG}
  
kubectl config use-context default --kubeconfig=${KUBE_CONFIG}

```

创建服务

```
cat > /usr/lib/systemd/system/kube-proxy.service << EOF
[Unit]
Description=Kubernetes Proxy
After=network.target

[Service]
    EnvironmentFile=/opt/kubernetes/cfg/kube-proxy.conf
ExecStart=/opt/kubernetes/bin/kube-proxy \$KUBE_PROXY_OPTS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

```

```
systemctl daemon-reload
systemctl enable kube-proxy
systemctl start kube-proxy
systemctl status kube-proxy

```

## （master）添加网络管理组件

下载calicotl

```
https://github.com/projectcalico/calico/releases/download/v3.25.0/calicoctl-linux-amd64
```

放到/usr/local/bin/中

```
mv /usr/local/bin/calicoctl-linux-amd64 /usr/local/bin/calicoctl
chmod +x /usr/local/bin/calicoctl
```

下载配置

```
https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml
```

放到/opt/kubernetes/cfg/中

```
kubectl apply -f /opt/kubernetes/cfg/calico.yaml
kubectl get pods -n kube-system
```

## （node）复制节点

master节点

```
cd /opt/kubernetes
tar zvcf kube.gz bin/ cfg/ ssl/

```

将/opt/kubernetes/下的kube.gz文件下载下来

子节点

```
mkdir /opt/kubernetes

```

将刚刚下载的kube.gz文件上传到/opt/kubernetes/

```
 tar -zxvf kube.gz
 
```

删除部分配置，每个节点不同

```
rm -f /opt/kubernetes/cfg/kubelet.kubeconfig 
rm -f /opt/kubernetes/ssl/kube-link/kubelet*

```

修改主机名称

---

```
vi /opt/kubernetes/cfg/kubelet.conf

```

修改：--hostname-override=k8s-node1

```
vi /opt/kubernetes/cfg/kube-proxy-config.yml

```

修改：hostnameOverride: k8s-node1

---

创建软连接（创建服务）kubelet与kube-proxy

```
cat > /usr/lib/systemd/system/kubelet.service << EOF
[Unit]
Description=Kubernetes Kubelet
After=docker.service

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kubelet.conf
ExecStart=/opt/kubernetes/bin/kubelet \$KUBELET_OPTS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
cat > /usr/lib/systemd/system/kube-proxy.service << EOF
[Unit]
Description=Kubernetes Proxy
After=network.target

[Service]
    EnvironmentFile=/opt/kubernetes/cfg/kube-proxy.conf
ExecStart=/opt/kubernetes/bin/kube-proxy \$KUBE_PROXY_OPTS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

```

这里可以开一个新的ssh专门打日志

```
 tail -f /var/log/messages
```

开启kubelet

```
systemctl daemon-reload
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet

```

开启kube-proxy

```
systemctl daemon-reload
systemctl enable kube-proxy
systemctl start kube-proxy
systemctl status kube-proxy

```

在Master上同意新的Node kubelet证书申请

```
kubectl get csr
kubectl certificate approve #后面是上面语句获取的NAME列的一行
```

查看节点

```
kubectl get nodes
```

有三个Ready的
